{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feature_1  Feature_2   Feature_3  Target\n",
      "0   94.870985  88.239326  101.497093       0\n",
      "1   97.684482  84.837474   90.892151       0\n",
      "2   94.648343  77.467282   87.646104       0\n",
      "3   94.635471  85.327735   99.851568       0\n",
      "4  104.397011  84.097116   98.211326       0\n",
      "    Feature_1  Feature_2  Feature_3  Target\n",
      "0   97.533783  88.329103  98.191966       0\n",
      "1  102.838058  78.783627  90.867559       0\n",
      "2   95.155724  86.679155  94.861119       0\n",
      "3  102.838797  87.504581  99.622361       0\n",
      "4   99.238078  86.704614  91.685225       0\n"
     ]
    }
   ],
   "source": [
    "data1_train = pd.read_csv('data1_train.csv')\n",
    "data1_test = pd.read_csv('data1_test.csv')\n",
    "\n",
    "print(data1_train.head())\n",
    "print(data1_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature_1   Feature_2  Target\n",
      "0   8.160646   88.799326       0\n",
      "1  31.149536  102.335826       0\n",
      "2  13.103383   92.902908       0\n",
      "3  15.950445   77.412565       0\n",
      "4  35.856965   94.441550       0\n",
      "   Feature_1  Feature_2  Target\n",
      "0  48.489576  81.609641       0\n",
      "1  26.069706  89.783100       0\n",
      "2  31.967447  88.005024       0\n",
      "3  44.957613  91.219129       0\n",
      "4  27.681870  87.381969       0\n"
     ]
    }
   ],
   "source": [
    "data2_train = pd.read_csv('data2_train.csv')\n",
    "data2_test = pd.read_csv('data2_test.csv')\n",
    "\n",
    "print(data2_train.head())\n",
    "print(data2_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_data(train_data, test_data):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train = scaler.fit_transform(train_data.iloc[:, :-1].values)\n",
    "\n",
    "    y_train = train_data.iloc[:, -1].values\n",
    "\n",
    "    X_test = scaler.transform(test_data.iloc[:, :-1].values)\n",
    "\n",
    "    y_test = test_data.iloc[:, -1].values\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train1, y_train1, X_test1, y_test1 = normalize_data(data1_train, data1_test)\n",
    "\n",
    "X_train2, y_train2, X_test2, y_test2 = normalize_data(data2_train, data2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    \n",
    "    np.random.seed(42)\n",
    "\n",
    "    W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "\n",
    "    W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "\n",
    "    A1 = np.tanh(Z1)\n",
    "\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    cache = (Z1, A1, Z2, A2)\n",
    "\n",
    "    return A2, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(A2, y):\n",
    "\n",
    "    m = y.shape[0]\n",
    "\n",
    "    loss = -np.sum(y * np.log(A2) + (1 - y) * np.log(1 - A2)) / m\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, y, cache, W1, W2):\n",
    "\n",
    "    m = X.shape[0]\n",
    "\n",
    "    Z1, A1, Z2, A2 = cache\n",
    "\n",
    "    dZ2 = A2 - y.reshape(-1, 1)\n",
    "\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "\n",
    "    dZ1 = dA1 * (1 - np.power(A1, 2))\n",
    "\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    \n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "    return dW1, db1, dW2, db2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X, y, hidden_size, num_iterations, learning_rate):\n",
    "\n",
    "    input_size = X.shape[1]\n",
    "\n",
    "    output_size = 1  \n",
    "\n",
    "    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        A2, cache = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "        loss = compute_loss(A2, y)\n",
    "\n",
    "        dW1, db1, dW2, db2 = backward_propagation(X, y, cache, W1, W2)\n",
    "\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            \n",
    "            print(f\"Iteration {i}: Loss = {loss}\")\n",
    "    \n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1, b1, W2, b2):\n",
    "\n",
    "    A2, _ = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "    predictions = (A2 > 0.5).astype(int)\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 554.5177329596024\n",
      "Iteration 100: Loss = 388.3065041225273\n",
      "Iteration 200: Loss = 283.6525940982714\n",
      "Iteration 300: Loss = 211.79198468263206\n",
      "Iteration 400: Loss = 172.60552449012738\n",
      "Iteration 500: Loss = 235.28160944113276\n",
      "Iteration 600: Loss = 429.17159810979837\n",
      "Iteration 700: Loss = 691.2984529157013\n",
      "Iteration 800: Loss = 986.7762065911041\n",
      "Iteration 900: Loss = 1309.3253561516167\n",
      "Training Accuracy: 56.00%\n",
      "Test Accuracy: 62.50%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    accuracy = np.mean(y_true == y_pred.flatten()) * 100\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "hidden_size = 10\n",
    "\n",
    "num_iterations = 1000\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "W1, b1, W2, b2 = train_neural_network(X_train1, y_train1, hidden_size, num_iterations, learning_rate)\n",
    "\n",
    "\n",
    "y_train_pred = predict(X_train1, W1, b1, W2, b2)\\\n",
    "\n",
    "y_test_pred = predict(X_test1, W1, b1, W2, b2)\n",
    "\n",
    "\n",
    "train_accuracy = evaluate_model(y_train1, y_train_pred)\n",
    "\n",
    "test_accuracy = evaluate_model(y_test1, y_test_pred)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 554.5177537742115\n",
      "Iteration 100: Loss = 534.0115782414023\n",
      "Iteration 200: Loss = 514.5063398992793\n",
      "Iteration 300: Loss = 495.95098295197016\n",
      "Iteration 400: Loss = 478.29622133021\n",
      "Iteration 500: Loss = 461.4946349786193\n",
      "Iteration 600: Loss = 445.5007352844173\n",
      "Iteration 700: Loss = 430.27100266964464\n",
      "Iteration 800: Loss = 415.76389963280167\n",
      "Iteration 900: Loss = 401.9398625804143\n",
      "Iteration 0: Loss = 554.5177537742115\n",
      "Iteration 100: Loss = 388.6193862709186\n",
      "Iteration 200: Loss = 285.31189940772015\n",
      "Iteration 300: Loss = 217.21870325232805\n",
      "Iteration 400: Loss = 168.66874561679901\n",
      "Iteration 500: Loss = 137.20953401355564\n",
      "Iteration 600: Loss = 144.87569113985617\n",
      "Iteration 700: Loss = 216.66452206692887\n",
      "Iteration 800: Loss = 342.55587588821174\n",
      "Iteration 900: Loss = 495.7284836863255\n",
      "Iteration 0: Loss = 554.5177537742115\n",
      "Iteration 100: Loss = 624.1517355935082\n",
      "Iteration 200: Loss = 2697.7062686625513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_18664\\2378563390.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.sum(y * np.log(A2) + (1 - y) * np.log(1 - A2)) / m\n",
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_18664\\2378563390.py:5: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -np.sum(y * np.log(A2) + (1 - y) * np.log(1 - A2)) / m\n",
      "c:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 300: Loss = nan\n",
      "Iteration 400: Loss = nan\n",
      "Iteration 500: Loss = nan\n",
      "Iteration 600: Loss = nan\n",
      "Iteration 700: Loss = nan\n",
      "Iteration 800: Loss = nan\n",
      "Iteration 900: Loss = nan\n",
      "Iteration 0: Loss = 554.5177329596024\n",
      "Iteration 100: Loss = 533.993603104511\n",
      "Iteration 200: Loss = 514.4696396438995\n",
      "Iteration 300: Loss = 495.89394279822176\n",
      "Iteration 400: Loss = 478.21641703102347\n",
      "Iteration 500: Loss = 461.388835532018\n",
      "Iteration 600: Loss = 445.3648780689102\n",
      "Iteration 700: Loss = 430.1001439961877\n",
      "Iteration 800: Loss = 415.55214281031954\n",
      "Iteration 900: Loss = 401.68026499612114\n",
      "Iteration 0: Loss = 554.5177329596024\n",
      "Iteration 100: Loss = 388.3065041225273\n",
      "Iteration 200: Loss = 283.6525940982714\n",
      "Iteration 300: Loss = 211.79198468263206\n",
      "Iteration 400: Loss = 172.60552449012738\n",
      "Iteration 500: Loss = 235.28160944113276\n",
      "Iteration 600: Loss = 429.17159810979837\n",
      "Iteration 700: Loss = 691.2984529157013\n",
      "Iteration 800: Loss = 986.7762065911041\n",
      "Iteration 900: Loss = 1309.3253561516167\n",
      "Iteration 0: Loss = 554.5177329596024\n",
      "Iteration 100: Loss = 1599.8438556142646\n",
      "Iteration 200: Loss = nan\n",
      "Iteration 300: Loss = nan\n",
      "Iteration 400: Loss = nan\n",
      "Iteration 500: Loss = nan\n",
      "Iteration 600: Loss = nan\n",
      "Iteration 700: Loss = nan\n",
      "Iteration 800: Loss = nan\n",
      "Iteration 900: Loss = nan\n",
      "Iteration 0: Loss = 554.5177432435039\n",
      "Iteration 100: Loss = 533.9990990358585\n",
      "Iteration 200: Loss = 514.4799804419296\n",
      "Iteration 300: Loss = 495.90876880862396\n",
      "Iteration 400: Loss = 478.23560363160783\n",
      "Iteration 500: Loss = 461.41246018590914\n",
      "Iteration 600: Loss = 445.3931977650055\n",
      "Iteration 700: Loss = 430.1335809385695\n",
      "Iteration 800: Loss = 415.59127667714114\n",
      "Iteration 900: Loss = 401.72583041115485\n",
      "Iteration 0: Loss = 554.5177432435039\n",
      "Iteration 100: Loss = 388.3590537875423\n",
      "Iteration 200: Loss = 283.8624631424957\n",
      "Iteration 300: Loss = 212.24452115944786\n",
      "Iteration 400: Loss = 171.1434921745718\n",
      "Iteration 500: Loss = 232.74510882515602\n",
      "Iteration 600: Loss = 447.00038172008806\n",
      "Iteration 700: Loss = 757.6363263969881\n",
      "Iteration 800: Loss = 1128.3085062456591\n",
      "Iteration 900: Loss = 1549.0640176474553\n",
      "Iteration 0: Loss = 554.5177432435039\n",
      "Iteration 100: Loss = 1930.5048766415555\n",
      "Iteration 200: Loss = nan\n",
      "Iteration 300: Loss = nan\n",
      "Iteration 400: Loss = nan\n",
      "Iteration 500: Loss = nan\n",
      "Iteration 600: Loss = nan\n",
      "Iteration 700: Loss = nan\n",
      "Iteration 800: Loss = nan\n",
      "Iteration 900: Loss = nan\n",
      "Best Hidden Size: 5\n",
      "Best Learning Rate: 0.1\n",
      "Best Test Accuracy: 68.00%\n"
     ]
    }
   ],
   "source": [
    "def grid_search_nn(X_train, y_train, X_test, y_test, hidden_sizes, learning_rates, num_iterations):\n",
    "\n",
    "    best_hidden_size = hidden_sizes[0]\n",
    "\n",
    "    best_learning_rate = learning_rates[0]\n",
    "    \n",
    "    best_accuracy = 0\n",
    "\n",
    "    for hidden_size in hidden_sizes:\n",
    "\n",
    "        for learning_rate in learning_rates:\n",
    "\n",
    "            W1, b1, W2, b2 = train_neural_network(X_train, y_train, hidden_size, num_iterations, learning_rate)\n",
    "\n",
    "            y_test_pred = predict(X_test, W1, b1, W2, b2)\n",
    "\n",
    "            accuracy = evaluate_model(y_test, y_test_pred)\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_hidden_size = hidden_size\n",
    "                best_learning_rate = learning_rate\n",
    "\n",
    "    return best_hidden_size, best_learning_rate, best_accuracy\n",
    "\n",
    "hidden_sizes = [5, 10, 15]\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "num_iterations = 1000\n",
    "\n",
    "best_hidden_size, best_learning_rate, best_accuracy = grid_search_nn(X_train1, y_train1, X_test1, y_test1, hidden_sizes, learning_rates, num_iterations)\n",
    "\n",
    "print(f'Best Hidden Size: {best_hidden_size}')\n",
    "print(f'Best Learning Rate: {best_learning_rate}')\n",
    "print(f'Best Test Accuracy: {best_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.4358 - loss: 0.0438\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5811 - loss: -2.4197\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5901 - loss: -6.4231\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5498 - loss: -16.8231\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5386 - loss: -29.5735 \n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5403 - loss: -47.3409  \n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5508 - loss: -71.3670 \n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5573 - loss: -99.4045\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5314 - loss: -136.1270 \n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5538 - loss: -165.0581\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5761 - loss: -203.3412 \n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5256 - loss: -273.0117 \n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5372 - loss: -314.7866 \n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5414 - loss: -385.9225\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5442 - loss: -453.7433 \n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5651 - loss: -472.9838  \n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5580 - loss: -564.0900\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5504 - loss: -664.2251 \n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5541 - loss: -728.8463  \n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5614 - loss: -750.8031  \n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5674 - loss: -884.2709   \n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5780 - loss: -954.8304 \n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5344 - loss: -1104.1516 \n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5563 - loss: -1174.4989  \n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5662 - loss: -1205.9996\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5543 - loss: -1378.9191\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5861 - loss: -1348.5902\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5760 - loss: -1515.7378\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5573 - loss: -1764.3699\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5637 - loss: -1806.0551\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5811 - loss: -1882.9320\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5660 - loss: -2093.7166  \n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5696 - loss: -2207.1619  \n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5579 - loss: -2530.7332\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5393 - loss: -2586.5076\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5852 - loss: -2510.3252  \n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5426 - loss: -2903.1465  \n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5589 - loss: -3022.3577 \n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5305 - loss: -3217.1980  \n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5605 - loss: -3229.0164 \n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5489 - loss: -3419.2637  \n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5470 - loss: -3599.8496 \n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5362 - loss: -4064.0410\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5565 - loss: -3764.7300  \n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5633 - loss: -4329.1748  \n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5395 - loss: -4586.7085  \n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5563 - loss: -4585.4463  \n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5598 - loss: -4357.8818\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5538 - loss: -5011.8706 \n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5285 - loss: -5789.6602 \n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7438 - loss: -886.3159 \n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7476 - loss: -1584.5989\n",
      "Scikit-Learn Training Accuracy 1: 55.00%\n",
      "Scikit-Learn Test Accuracy 1: 61.50%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "def compare_with_tensorflow(X_train, y_train, X_test, y_test, hidden_size, learning_rate, num_iterations):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(hidden_size, input_dim=X_train.shape[1], activation='relu'))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(loss=BinaryCrossentropy(from_logits=True), optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=num_iterations)\n",
    "    \n",
    "    _, train_accuracy_tf = model.evaluate(X_train, y_train)\n",
    "    _, test_accuracy_tf = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    train_accuracy_tf *= 100\n",
    "    test_accuracy_tf *= 100\n",
    "\n",
    "    return train_accuracy_tf, test_accuracy_tf\n",
    "\n",
    "train_accuracy_tf, test_accuracy_tf = compare_with_tensorflow(X_train1,y_train1,X_test1,y_test1,30,0.01,50)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Scikit-Learn Training Accuracy 1: {train_accuracy_tf:.2f}%')\n",
    "\n",
    "print(f'Scikit-Learn Test Accuracy 1: {test_accuracy_tf:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
